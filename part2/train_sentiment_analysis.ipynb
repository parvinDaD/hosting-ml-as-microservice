{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 2: Training your own ML Model\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/peckjon/hosting-ml-as-microservice/blob/master/part2/train_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download corpuses\n",
    "\n",
    "We'll continue using the `movie_reviews` corpus to train our model. The `stopwords` corpus contains a [set of standard stopwords](https://gist.github.com/sebleier/554280) we'll want to remove from the input, and `punkt` is used for toneization in the [.words()](https://www.nltk.org/api/nltk.corpus.html#corpus-reader-functions) method of the corpus reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/parvindadgar/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/parvindadgar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/parvindadgar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import download\n",
    "\n",
    "download('movie_reviews')\n",
    "download('punkt')\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    " \n",
    "def read_in(folder):\n",
    "    files = os.listdir(folder)\n",
    "    a_list = []\n",
    "    for a_file in files:\n",
    "        if not a_file.startswith(\".\"):\n",
    "            f = codecs.open(folder + a_file, \n",
    "                \"r\", encoding = \"ISO-8859-1\", errors=\"ignore\")\n",
    "            a_list.append(f.read())\n",
    "            f.close()\n",
    "    return a_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "3672\n",
      "Subject: what up , , your cam babe\r\n",
      "what are you looking for ?\r\n",
      "if your looking for a companion for friendship , love , a date , or just good ole '\r\n",
      "fashioned * * * * * * , then try our brand new site ; it was developed and created\r\n",
      "to help anyone find what they ' re looking for . a quick bio form and you ' re\r\n",
      "on the road to satisfaction in every sense of the word . . . . no matter what\r\n",
      "that may be !\r\n",
      "try it out and youll be amazed .\r\n",
      "have a terrific time this evening\r\n",
      "copy and pa ste the add . ress you see on the line below into your browser to come to the site .\r\n",
      "http : / / www . meganbang . biz / bld / acc /\r\n",
      "no more plz\r\n",
      "http : / / www . naturalgolden . com / retract /\r\n",
      "counterattack aitken step preemptive shoehorn scaup . electrocardiograph movie honeycomb . monster war brandywine pietism byrne catatonia . encomia lookup intervenor skeleton turn catfish .\r\n",
      "\n",
      "Subject: ena sales on hpl\r\n",
      "just to update you on this project ' s status :\r\n",
      "based on a new report that scott mills ran for me from sitara , i have come up\r\n",
      "with the following counterparties as the ones to which ena is selling gas off\r\n",
      "of hpl ' s pipe .\r\n",
      "altrade transaction , l . l . c . gulf gas utilities company\r\n",
      "brazoria , city of panther pipeline , inc .\r\n",
      "central illinois light company praxair , inc .\r\n",
      "central power and light company reliant energy - entex\r\n",
      "ces - equistar chemicals , lp reliant energy - hl & p\r\n",
      "corpus christi gas marketing , lp southern union company\r\n",
      "d & h gas company , inc . texas utilities fuel company\r\n",
      "duke energy field services , inc . txu gas distribution\r\n",
      "entex gas marketing company union carbide corporation\r\n",
      "equistar chemicals , lp unit gas transmission company inc .\r\n",
      "since i ' m not sure exactly what gets entered into sitara , pat clynes\r\n",
      "suggested that i check with daren farmer to make sure that i ' m not missing\r\n",
      "something ( which i did below ) . while i am waiting for a response from him\r\n",
      "and / or mary smith , i will begin gathering the contractual volumes under the\r\n",
      "above contracts .\r\n",
      "- - - - - - - - - - - - - - - - - - - - - - forwarded by cheryl dudley / hou / ect on 05 / 10 / 2000 07 : 56\r\n",
      "am - - - - - - - - - - - - - - - - - - - - - - - - - - -\r\n",
      "cheryl d king\r\n",
      "05 / 08 / 2000 04 : 11 pm\r\n",
      "sent by : cheryl dudley\r\n",
      "to : daren j farmer / hou / ect @ ect , mary m smith / hou / ect @ ect\r\n",
      "cc :\r\n",
      "subject : ena sales on hpl\r\n",
      "i am working on a project for brenda herod & was wondering if one of you\r\n",
      "could tell me if i ' m on the right track & if this will get everything for\r\n",
      "which she is looking .\r\n",
      "she is trying to draft a long - term transport / storage agreement between ena &\r\n",
      "hplc which will allow ena to move the gas to their markets . in order to\r\n",
      "accomplish this , she needs to know all of the sales to customers that ena is\r\n",
      "doing off of hpl ' s pipe .\r\n",
      "i had scott mills run a report from sitara showing all ena buy / sell activity\r\n",
      "on hpl since 7 / 99 . if i eliminate the buys & the desk - to - desk deals , will\r\n",
      "this give me everything that i need ?\r\n",
      "are there buy / sell deals done with ena on hpl ' s pipe that wouldn ' t show up in\r\n",
      "sitara ? someone mentioned something about deals where hpl transports the gas\r\n",
      "on it ' s own behalf then ena sells it to a customer at that same spot - -\r\n",
      "? ? ? ? ? do deals like that happen ? would they show up in sitara ?\r\n",
      "is there anything else that i ' m missing ? i ' m not real familiar with how some\r\n",
      "of these deals happen nowadays so am very receptive to any\r\n",
      "ideas / suggestions / help that you can offer ! ! !\r\n",
      "thanks in advance .\n"
     ]
    }
   ],
   "source": [
    "spam_list = read_in(\"enron1/spam/\") \n",
    "ham_list = read_in(\"enron1/ham/\")\n",
    "print(len(spam_list)) \n",
    "print(len(ham_list))\n",
    "print(spam_list[0])\n",
    "print(ham_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size = 5172 emails\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# make it as tuple of content, label\n",
    " \n",
    "all_emails = [(email_content, \"spam\") for email_content in spam_list]\n",
    "all_emails += [(email_content, \"ham\") for email_content in ham_list]\n",
    "random.seed(42)\n",
    "random.shuffle(all_emails)\n",
    "print (f\"Dataset size = {str(len(all_emails))} emails\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size = 4137 emails\n",
      "Test set size = 1035 emails\n"
     ]
    }
   ],
   "source": [
    "from nltk import NaiveBayesClassifier, classify\n",
    " \n",
    "def train(features, proportion):\n",
    "    train_size = int(len(features) * proportion)\n",
    "    train_set, test_set = features[:train_size], features[train_size:]\n",
    "    print (f\"Training set size = {str(len(train_set))} emails\")\n",
    "    print (f\"Test set size = {str(len(test_set))} emails\")\n",
    "    classifier = NaiveBayesClassifier.train(train_set)\n",
    "    return train_set, test_set, classifier\n",
    " \n",
    "train_set, test_set, classifier = train(all_features, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the training set = 0.9635001208605269\n",
      "Accuracy of the test set = 0.9371980676328503\n",
      "Most Informative Features\n",
      "               forwarded = True              ham : spam   =    200.5 : 1.0\n",
      "                     hou = True              ham : spam   =    186.6 : 1.0\n",
      "                    2004 = True             spam : ham    =    163.0 : 1.0\n",
      "                     nom = True              ham : spam   =    126.0 : 1.0\n",
      "                    pain = True             spam : ham    =    103.6 : 1.0\n",
      "                    spam = True             spam : ham    =     92.4 : 1.0\n",
      "                     ect = True              ham : spam   =     83.3 : 1.0\n",
      "                  health = True             spam : ham    =     81.1 : 1.0\n",
      "                     sex = True             spam : ham    =     81.1 : 1.0\n",
      "              nomination = True              ham : spam   =     76.8 : 1.0\n",
      "                   super = True             spam : ham    =     74.7 : 1.0\n",
      "                featured = True             spam : ham    =     73.1 : 1.0\n",
      "                  differ = True             spam : ham    =     71.5 : 1.0\n",
      "                creative = True             spam : ham    =     71.5 : 1.0\n",
      "                     ibm = True             spam : ham    =     63.4 : 1.0\n",
      "                   adobe = True             spam : ham    =     61.8 : 1.0\n",
      "                  shares = True             spam : ham    =     61.8 : 1.0\n",
      "            solicitation = True             spam : ham    =     61.8 : 1.0\n",
      "             medications = True             spam : ham    =     60.2 : 1.0\n",
      "               clearance = True             spam : ham    =     60.2 : 1.0\n",
      "                    2001 = True              ham : spam   =     59.4 : 1.0\n",
      "                congress = True             spam : ham    =     58.6 : 1.0\n",
      "                    2005 = True             spam : ham    =     57.3 : 1.0\n",
      "                     pro = True             spam : ham    =     57.0 : 1.0\n",
      "                      cc = True              ham : spam   =     56.3 : 1.0\n",
      "                    draw = True             spam : ham    =     55.4 : 1.0\n",
      "                 dealers = True             spam : ham    =     55.4 : 1.0\n",
      "              complaints = True             spam : ham    =     53.8 : 1.0\n",
      "                   cheap = True             spam : ham    =     51.6 : 1.0\n",
      "                  farmer = True              ham : spam   =     46.1 : 1.0\n",
      "                  sexual = True             spam : ham    =     45.8 : 1.0\n",
      "                 doctors = True             spam : ham    =     45.8 : 1.0\n",
      "               publisher = True             spam : ham    =     45.8 : 1.0\n",
      "               thousands = True             spam : ham    =     44.2 : 1.0\n",
      "                mailings = True             spam : ham    =     42.6 : 1.0\n",
      "              affordable = True             spam : ham    =     42.6 : 1.0\n",
      "                reliable = True             spam : ham    =     42.6 : 1.0\n",
      "                licensed = True             spam : ham    =     42.6 : 1.0\n",
      "                   julie = True              ham : spam   =     42.2 : 1.0\n",
      "                      ex = True             spam : ham    =     41.9 : 1.0\n",
      "                 advises = True             spam : ham    =     41.0 : 1.0\n",
      "                 popular = True             spam : ham    =     41.0 : 1.0\n",
      "                pipeline = True              ham : spam   =     40.0 : 1.0\n",
      "                    lisa = True              ham : spam   =     40.0 : 1.0\n",
      "                powerful = True             spam : ham    =     39.4 : 1.0\n",
      "                      wi = True             spam : ham    =     39.4 : 1.0\n",
      "                 foresee = True             spam : ham    =     39.4 : 1.0\n",
      "                   risks = True             spam : ham    =     39.0 : 1.0\n",
      "                     ali = True             spam : ham    =     39.0 : 1.0\n",
      "                   steve = True              ham : spam   =     38.6 : 1.0\n"
     ]
    }
   ],
   "source": [
    "def evaluate(train_set, test_set, classifier):\n",
    "    print (f\"Accuracy on the training set = {str(classify.accuracy(classifier, train_set))}\")\n",
    "    print (f\"Accuracy of the test set = {str(classify.accuracy(classifier, test_set))}\")\n",
    "    classifier.show_most_informative_features(50)\n",
    "evaluate(train_set, test_set, classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define feature extractor and bag-of-words converter\n",
    "\n",
    "Given a list of (already tokenized) words, we need a function to extract just the ones we care about: those not found in the list of English stopwords or standard punctuation.\n",
    "\n",
    "We also need a way to easily turn a list of words into a [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model), pairing each word with the count of its occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOCKS in HAM:\n",
      "Displaying 1 of 1 matches:\n",
      "ur member directory . * follow your stocks and news headlines , exchange files\n",
      "Displaying 1 of 1 matches:\n",
      "ur member directory . * follow your stocks and news headlines , exchange files\n",
      "Displaying 1 of 1 matches:\n",
      "ur member directory . * follow your stocks and news headlines , exchange files\n",
      "Displaying 1 of 1 matches:\n",
      "ad my portfolio is diversified into stocks that have lost even more money than\n",
      "\n",
      "\n",
      "STOCKS in SPAM:\n",
      "Displaying 3 of 3 matches:\n",
      "report reveals this smallcap rocket stocks newsletter first we would like to s\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 3 of 3 matches:\n",
      "might occur . as with many microcap stocks , today ' s company has additional \n",
      "is emai | pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this emai | . none \n",
      "Displaying 6 of 6 matches:\n",
      "hem : ( big money was made in these stocks by savvy investors who timed them r\n",
      "g filthy , stinking ri ' ch in tiny stocks no one has ever heard of until now \n",
      "ynamic things . some of these small stocks have absolutely exploded in price r\n",
      "`` occur . as with many micro - cap stocks , today ' s company has additional \n",
      " ema - il pertaining to investing , stocks or securities must be understood as\n",
      "ntative before deciding to trade in stocks featured within this ema - il . non\n",
      "Displaying 2 of 2 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this emai | . none \n",
      "Displaying 1 of 1 matches:\n",
      "in apple investments , inc profiled stocks . in order to be in full compliance\n",
      "Displaying 3 of 3 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      " lose money from investing in penny stocks . if you wish to stop future mailin\n",
      "Displaying 3 of 3 matches:\n",
      " plays . widespread gains in energy stocks are inflating the portfolios of agg\n",
      "st levels of the year , with energy stocks outperforming all other market sect\n",
      "utions that sma | | and micro - cap stocks are high - risk investments and tha\n",
      "Displaying 4 of 4 matches:\n",
      "watch this one trade . these little stocks can surprise in a big way sometimes\n",
      "might occur . as with many microcap stocks , today ' s company has additional \n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 1 of 1 matches:\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ penny - stocks are considered highly speculative a\n",
      "Displaying 3 of 3 matches:\n",
      "might occur . as with many microcap stocks , today ' s company has additiona |\n",
      "is emai | pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 1 of 1 matches:\n",
      "ne trade thursday ! go fcdh . penny stocks are considered highiy specuiative a\n",
      "Displaying 2 of 2 matches:\n",
      "ims and do your own due diligence . stocks to play ( s 2 p ) profiles are not \n",
      "s obtained . investing in micro cap stocks is extremely risky and , investors \n",
      "Displaying 2 of 2 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 2 of 2 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 1 of 1 matches:\n",
      "scovering value in natural resource stocks elgin resources ( elr - tsx ) extra\n",
      "Displaying 2 of 2 matches:\n",
      "ck monday some of these little voip stocks have been really moving lately . an\n",
      " one trade monday ! go ypil . penny stocks are considered highiy specuiative a\n",
      "Displaying 1 of 1 matches:\n",
      " one trade monday ! go ndin . penny stocks are considered highly speculative a\n",
      "Displaying 1 of 1 matches:\n",
      " one trade monday ! go wysk . penny stocks are considered highiy specuiative a\n",
      "Displaying 1 of 1 matches:\n",
      "fessionally not multi - level - not stocks - not real estate no cost tele - se\n",
      "Displaying 2 of 2 matches:\n",
      " % on regular price we have massive stocks of drugs for same day dispatch fast\n",
      "e do have the lowest price and huge stocks ready for same - day dispatch . two\n",
      "Displaying 5 of 5 matches:\n",
      "ck monday some of these littie voip stocks have been really moving lately . an\n",
      "t can happen with these sma | | cap stocks when they take off . and it happens\n",
      " statements . as with many microcap stocks , today ' s company has additiona |\n",
      "is report pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 3 of 3 matches:\n",
      "might occur . as with many microcap stocks , today ' s company has additiona |\n",
      "is emai | pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this emai | . none \n",
      "Displaying 1 of 1 matches:\n",
      "subject : fwd : screw doctors . stocks available . vlagr @ . x _ a _ nax .\n",
      "Displaying 1 of 1 matches:\n",
      " one trade monday ! go wysk . penny stocks are considered highiy speculative a\n",
      "Displaying 2 of 2 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 4 of 4 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "eep in mind that when trading small stocks like the company above there is a c\n",
      "t professional before investing any stocks or mutual funds .\n",
      "Displaying 1 of 1 matches:\n",
      " one trade monday ! go wysk . penny stocks are considered highiy specuiative a\n",
      "Displaying 1 of 1 matches:\n",
      "dge - ksige are you tired of buying stocks and not having them perform ? our s\n",
      "Displaying 4 of 4 matches:\n",
      "ck monday some of these little voip stocks have been rea | | y moving lately .\n",
      " statements . as with many microcap stocks , today ' s company has additiona |\n",
      "is report pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 4 of 4 matches:\n",
      "tion is key to stock success rocket stocks newsletter u r g e n t i n v e s t \n",
      "ht occur . as with many micro - cap stocks , today ' s company has additional \n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 4 of 4 matches:\n",
      " the last 12 months , many of these stocks made triple and even quadruple retu\n",
      " statements . as with many microcap stocks , today ' s company has additiona |\n",
      "is report pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 3 of 3 matches:\n",
      "ancements but may be one of the few stocks left in this industry group that is\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 3 of 3 matches:\n",
      "might occur . as with many microcap stocks , today ' s company has additiona |\n",
      "is emai | pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 2 of 2 matches:\n",
      " % on regular price we have massive stocks of drugs for same day dispatch fast\n",
      "e do have the lowest price and huge stocks ready for same - day dispatch . two\n",
      "Displaying 1 of 1 matches:\n",
      "or information puposes only . penny stocks are considered highly speculative a\n",
      "Displaying 4 of 4 matches:\n",
      "hree days . play of the week tracks stocks on downward trends , foresees botto\n",
      "mark is our uncanny ability to spot stocks that have bottomed - out and antici\n",
      "ound and upward trend . most of the stocks we track rebound and peak within ju\n",
      "om third party . investing in penny stocks is high risk and you should seek pr\n",
      "Displaying 4 of 4 matches:\n",
      "nt opportunity drummond , small cap stocks alert newsletter must read - alert \n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      " lose money from investing in penny stocks . - - - - - - - - - - - - - - - - -\n",
      "Displaying 4 of 4 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "eep in mind that when trading small stocks like the company above there is a c\n",
      "t professional before investing any stocks or mutual funds .\n",
      "Displaying 1 of 1 matches:\n",
      "the | ast 12 months , many of these stocks made triple and even quadruple retu\n",
      "Displaying 1 of 1 matches:\n",
      "cautions that small and micro - cap stocks are high - risk investments and tha\n",
      "Displaying 4 of 4 matches:\n",
      "k tuesday some of these littie voip stocks have been reaily moving lateiy . an\n",
      " statements . as with many microcap stocks , today ' s company has additional \n",
      "is report pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 2 of 2 matches:\n",
      "ck monday some of these little voip stocks have been realiy moving lately . an\n",
      " one trade monday ! go ypil . penny stocks are considered highiy specuiative a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 2 of 2 matches:\n",
      " % on regular price we have massive stocks of drugs for same day dispatch fast\n",
      "e do have the lowest price and huge stocks ready for same - day dispatch . two\n",
      "Displaying 2 of 2 matches:\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 4 of 4 matches:\n",
      "n this stock . some of these smal | stocks are absoiuteiy fiying , as many of \n",
      " statements . as with many microcap stocks , todays company has additional ris\n",
      "biication pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this publication . \n",
      "Displaying 1 of 1 matches:\n",
      "s obtained . investing in micro cap stocks is extremely risky and , investors \n",
      "Displaying 2 of 2 matches:\n",
      "is emai | pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 3 of 3 matches:\n",
      " statements . as with many microcap stocks , todays company has additional ris\n",
      "blication pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this publication . \n",
      "Displaying 1 of 1 matches:\n",
      " the last 12 months , many of these stocks made tripie and even quadruple retu\n",
      "Displaying 3 of 3 matches:\n",
      "orage inc . play of the week tracks stocks on downward trends , foresees botto\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 1 of 1 matches:\n",
      "ecializing in undervalued small cap stocks for immediate breakout erhc and exx\n",
      "Displaying 3 of 3 matches:\n",
      "5 how many times have you seen good stocks but you couldn ' t get your hands o\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 5 of 5 matches:\n",
      "5 where were you when the following stocks exploded : scos : exploded from . 3\n",
      "d . 80 on friday . face it . little stocks can mean big gains for you . this r\n",
      "might occur . as with many microcap stocks , today ' s company has additional \n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 2 of 2 matches:\n",
      "subject : penny stocks are about timing nomad internationa\n",
      " one trade friday ! go ndin . penny stocks are considered highiy speculative a\n",
      "Displaying 2 of 2 matches:\n",
      "ng their gains . select gold mining stocks are the hot flyers of the otc . his\n",
      "is letter cautions that micro - cap stocks are high - risk investments and tha\n",
      "Displaying 1 of 1 matches:\n",
      "cautions that small and micro - cap stocks are high - risk investments and tha\n",
      "Displaying 5 of 5 matches:\n",
      "hursday ! some of these littie voip stocks have been realiy moving lateiy . an\n",
      "t can happen with these sma | | cap stocks when they take off . and it happens\n",
      " statements . as with many microcap stocks , today ' s company has additiona |\n",
      "is report pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 2 of 2 matches:\n",
      "rt identifying defense and security stocks ready to explode look at the moves \n",
      " actual exchanges where small - cap stocks are traded . silica stopband doorkn\n",
      "Displaying 6 of 6 matches:\n",
      " if you knew about these low priced stocks : otcbb : zapz : closed march 31 st\n",
      " following points : * many of these stocks are undiscovered and uncovered ! wh\n",
      " ! ! * * many of these undiscovered stocks are like coiled springs , wound tig\n",
      "might occur . as with many microcap stocks , today ' s company has additional \n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 2 of 2 matches:\n",
      " the last 12 months , many of these stocks made tripie and even quadruple retu\n",
      "one trade tuesday ! go mogi . penny stocks are considered highly speculative a\n",
      "Displaying 3 of 3 matches:\n",
      " statements . as with many microcap stocks , todays company has additional ris\n",
      "blication pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this publication . \n",
      "Displaying 3 of 3 matches:\n",
      " statements . as with many microcap stocks , today ' s company has additiona |\n",
      "is report pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 4 of 4 matches:\n",
      "y agree , some , not all , of these stocks move in price because they are prom\n",
      "tands or that as with many microcap stocks , today ' s company has additional \n",
      "is report pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this report . none \n",
      "Displaying 3 of 3 matches:\n",
      "might occur . as with many microcap stocks , today ' s company has additiona |\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 3 of 3 matches:\n",
      "n how many times have you seen good stocks but you couldn ' t get your hands o\n",
      "his email pertaining to investing , stocks , securities must be understood as \n",
      "ntative before deciding to trade in stocks featured within this email . none o\n",
      "Displaying 1 of 1 matches:\n",
      " receive first notice on run - away stocks traders ' monthly alert january pic\n"
     ]
    }
   ],
   "source": [
    "from nltk.text import Text\n",
    " \n",
    "def concordance(data_list, search_word):\n",
    "    for email in data_list:\n",
    "        word_list = [word for word in word_tokenize(email.lower())]\n",
    "        text_list = Text(word_list)\n",
    "        if search_word in word_list:\n",
    "            text_list.concordance(search_word)\n",
    " \n",
    " \n",
    "print (\"STOCKS in HAM:\")\n",
    "concordance(ham_list, \"stocks\")\n",
    "print (\"\\n\\nSTOCKS in SPAM:\")\n",
    "concordance(spam_list, \"stocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', \"'s\", 'the', 'best', 'way', 'to', 'split', 'a', 'sentence', 'into', 'words']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    " \n",
    "def tokenize(input):\n",
    "    word_list = []\n",
    "    for word in word_tokenize(input):\n",
    "        word_list.append(word)\n",
    "    return word_list\n",
    "    \n",
    "input = \"What's the best way to split a sentence into words\"\n",
    "print(tokenize(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'participate': True, 'in': True, 'our': True, 'new': True, 'lottery': True, 'now': True, '!': True}\n",
      "5172\n",
      "37\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "def get_features(text):\n",
    "    features = {}\n",
    "    word_list = [word for word in word_tokenize(text.lower())]\n",
    "    for word in word_list:\n",
    "        features[word] = True\n",
    "    return features\n",
    " \n",
    "all_features = [(get_features(email), label) \n",
    "                 for (email, label) in all_emails]\n",
    " \n",
    "print(get_features(\"Participate In Our New Lottery NOW!\"))\n",
    "print(len(all_features))\n",
    "print(len(all_features[0][0]))\n",
    "print(len(all_features[99][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "stopwords_eng = stopwords.words('english')\n",
    "\n",
    "def extract_features(words):\n",
    "    return [w for w in words if w not in stopwords_eng and w not in punctuation]\n",
    "\n",
    "def bag_of_words(words):\n",
    "    bag = {}\n",
    "    for w in words:\n",
    "        bag[w] = bag.get(w,0)+1\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest, clean, and convert the positive and negative reviews\n",
    "\n",
    "For both the positive (\"pos\") and negative (\"neg\") sets of reviews, extract the features and convert to bag of words. From these, we construct a list of tuples known as a \"featureset\": the first part of each tuple is the bag of words for that review, and the second is its label (\"pos\"/\"neg\").\n",
    "\n",
    "Note that `movie_reviews.words(fileid)` provides a tokenized list of words. If we wanted the un-tokenized text, we would use `movie_reviews.raw(fileid)` instead, then tokenize it using our preferred tokenizeer (e.g. [nltk.tokenize.word_tokenize](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktLanguageVars.word_tokenize))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "reviews_pos = []\n",
    "reviews_neg = []\n",
    "for fileid in movie_reviews.fileids('pos'):\n",
    "    words = extract_features(movie_reviews.words(fileid))\n",
    "    reviews_pos.append((bag_of_words(words), 'pos'))\n",
    "for fileid in movie_reviews.fileids('neg'):\n",
    "    words = extract_features(movie_reviews.words(fileid))\n",
    "    reviews_neg.append((bag_of_words(words), 'neg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split reviews into training and test sets\n",
    "We need to break up each group of reviews into a training set (about 80%) and a test set (the remaining 20%). In case there's some meaningful order to the reviews (e.g. the first 800 are from one group of reviewers, the next 200 are from another), we shuffle the sets first to ensure we aren't introducing additional bias. Note that this means our accuracy will not be exactly the same on every run; if you wish to see consistent results on each run, you can stabilize the shuffle by calling [random.seed(n)](https://www.geeksforgeeks.org/random-seed-in-python/) first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "split_pct = .80\n",
    "\n",
    "def split_set(review_set):\n",
    "    split = int(len(review_set)*split_pct)\n",
    "    return (review_set[:split], review_set[split:])\n",
    "\n",
    "shuffle(reviews_pos)\n",
    "shuffle(reviews_neg)\n",
    "\n",
    "pos_train, pos_test = split_set(reviews_pos)\n",
    "neg_train, neg_test = split_set(reviews_neg)\n",
    "\n",
    "train_set = pos_train+neg_train\n",
    "test_set = pos_test+neg_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Now that our data is ready, the training step itself is quite simple if we use the [NaiveBayesClassifier](https://www.nltk.org/api/nltk.classify.html#module-nltk.classify.naivebayes) provided by NLTK.\n",
    "\n",
    "If you are used to methods such as `model.fit(x,y)` which take two parameters -- the data and the labels -- it may be confusing that `NaiveBayesClassifier.train` takes just one argument. This is because the labels are already embedded in `train_set`: each element in the set is a Bag of Words paired with a 'pos' or 'neg'; value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "model = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model accuracy\n",
    "\n",
    "NLTK's built-in [accuracy](https://www.nltk.org/api/nltk.classify.html#module-nltk.classify.util) utility can run our test_set through the model and compare the labels returned by the model to the labels in the test set, producing an overall % accuracy. Not too impressive, right? We need to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify.util import accuracy\n",
    "\n",
    "print(100 * accuracy(model, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model\n",
    "Our trained model will be cleared from memory when this notebook is closed. So that we can use it again later, save the model as a file using the [pickle](https://docs.python.org/3/library/pickle.html) serializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_file = open('sa_classifier.pickle','wb')\n",
    "pickle.dump(model, model_file)\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model (Colab version)\n",
    "\n",
    "Google Colab doesn't provide direct access to files saved during a notebook session, so we need to save it in [Google Drive](https://drive.google.com) instead. The first time you run this, it will ask for permission to access your Google Drive. Follow the instructions, then wait a few minutes and look for a new folder called \"Colab Output\" in [Drive](https://drive.google.com). Note that Colab does not alway sync to Drive immediately, so check the file update times and re-run this cell if it doesn't look like you have the most revent version of your file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    !mkdir -p '/content/gdrive/My Drive/Colab Output'\n",
    "    model_file = open('/content/gdrive/My Drive/Colab Output/sa_classifier.pickle','wb')\n",
    "    pickle.dump(model, model_file)\n",
    "    model_file.flush()\n",
    "    print('Model saved in /content/gdrive/My Drive/Colab Output')\n",
    "    !ls '/content/gdrive/My Drive/Colab Output'\n",
    "    drive.flush_and_unmount()\n",
    "    print('Re-run this cell if you cannot find it in https://drive.google.com')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
